from torch.nn.modules.loss import  _Loss
from torch.autograd import Variable
import torch
import numpy as np


def calculate_means(pred, gt, n_objects, max_n_objects, usegpu):
    """pred: bs, height * width, n_filters
       gt: bs, height * width, n_instances"""
    #print(torch.sum(pred),torch.sum(gt))
    bs, n_loc, n_filters = pred.size()
    n_instances = gt.size(2)

    pred_repeated = pred.unsqueeze(2).expand(
        bs, n_loc, n_instances, n_filters)  # bs, n_loc, n_instances, n_filters
    # bs, n_loc, n_instances, 1
    gt_expanded = gt.unsqueeze(3)

    pred_masked = pred_repeated * gt_expanded

    means = []
    for i in range(bs):
        _n_objects_sample = n_objects[i]
        # n_loc, n_objects, n_filters
        _pred_masked_sample = pred_masked[i, :, : _n_objects_sample]
        #print(torch.sum(_pred_masked_sample))
        #print(torch.sum(gt))
        # n_loc, n_objects, 1
        _gt_expanded_sample = gt_expanded[i, :, : _n_objects_sample]

        _mean_sample = _pred_masked_sample.sum(
            0) / _gt_expanded_sample.sum(0)  # n_objects, n_filters
        if (max_n_objects - _n_objects_sample) != 0:
            n_fill_objects = int(max_n_objects - _n_objects_sample)
            _fill_sample = torch.zeros(n_fill_objects, n_filters)
            if usegpu:
                _fill_sample = _fill_sample.cuda()
            _mean_sample = torch.cat((_mean_sample, _fill_sample), dim=0)
        means.append(_mean_sample)

    means = torch.stack(means)

    #means = pred_masked.sum(1) / gt_expanded.sum(1)
    # # bs, n_instances, n_filters
    #print("means:",means.size())
    return means

'''def calculate_means(pred, gt, n_objects, max_n_objects, usegpu):
    """pred: bs, height * width, n_filters
       gt: bs, height * width, n_instances"""
    #print(torch.sum(pred),torch.sum(gt))
    bs, n_loc, n_filters = pred.size()
    n_instances = gt.size(2)

    #pred_repeated = pred.unsqueeze(2).expand(
    #    bs, n_loc, n_instances, n_filters)  # bs, n_loc, n_instances, n_filters
    pred_repeated = pred.unsqueeze(2)
      # bs, n_loc, 1, n_filters
    
    # bs, n_loc, n_instances, 1
    gt_expanded = gt.unsqueeze(3)

    pred_masked = pred_repeated * gt_expanded
    # bs, n_loc, n_instances,n_filters

    means = pred_masked.sum(1) / (gt_expanded.sum(1) + 1e-10)
    # bs,n_instances,n_filters
    return means'''


def calculate_variance_term(pred, gt, means, n_objects, delta_v, norm=2):
    """pred: bs, height * width, n_filters
       gt: bs, height * width, n_instances
       means: bs, n_instances, n_filters"""
    means = means.detach()
    bs, n_loc, n_filters = pred.size()
    n_instances = gt.size(2)

    # bs, n_loc, n_instances, n_filters
    means = means.unsqueeze(1).expand(bs, n_loc, n_instances, n_filters)
    # bs, n_loc, n_instances, n_filters
    pred = pred.unsqueeze(2).expand(bs, n_loc, n_instances, n_filters)
    # bs, n_loc, n_instances, n_filters
    gt = gt.unsqueeze(3).expand(bs, n_loc, n_instances, n_filters)

    _var = (torch.clamp(torch.norm((pred - means), norm, 3) -
                        delta_v, min=0.0) ** 2) * gt[:, :, :, 0]

    var_term = 0.0
    for i in range(bs):
        _var_sample = _var[i, :, :n_objects[i]]  # n_loc, n_objects
        _gt_sample = gt[i, :, :n_objects[i], 0]  # n_loc, n_objects

        var_term += torch.sum(_var_sample) / torch.sum(_gt_sample)
    var_term = var_term / bs

    return var_term

'''def calculate_variance_term(pred, gt, means, n_objects, delta_v, norm=2):
    """pred: bs, height * width, n_filters
       gt: bs, height * width, n_instances
       means: bs, n_instances, n_filters"""
    means = means.detach()
    bs, n_loc, n_filters = pred.size()
    n_instances = gt.size(2)

    # bs, 1, n_instances, n_filters
    #means = means.unsqueeze(1).expand(bs, n_loc, n_instances, n_filters)
    means = means.unsqueeze(1)

    # bs, n_loc, 1, n_filters
    #pred = pred.unsqueeze(2).expand(bs, n_loc, n_instances, n_filters)
    pred = pred.unsqueeze(2)
    
    # bs, n_loc, n_instances
    #gt = gt.unsqueeze(3).expand(bs, n_loc, n_instances, n_filters)
    gt = gt 

    _var = (torch.clamp(torch.norm((pred - means), norm, 3) -
                        delta_v, min=0.0) ** 2) * gt
    #bs , n_loc, n_instances

    var_term = torch.sum(_var[gt > 0])/(torch.sum( gt ) + 1e-10)

    return var_term'''

def calculate_distance_term(means, n_objects, delta_d, norm=2, usegpu=True):
    """means: bs, n_instances, n_filters"""

    bs, n_instances, n_filters = means.size()

    dist_term = 0.0
    

    for i in range(bs):
        _n_objects_sample = int(n_objects[i])

        if _n_objects_sample <= 1:
            continue

        _mean_sample = means[i, : _n_objects_sample, :]  # n_objects, n_filters
        means_1 = _mean_sample.unsqueeze(1).expand(
            _n_objects_sample, _n_objects_sample, n_filters)
        means_2 = means_1.permute(1, 0, 2)

        diff = means_1 - means_2  # n_objects, n_objects, n_filters

        _norm = torch.norm(diff, norm, 2)

        margin = 2 * delta_d * (1.0 - torch.eye(_n_objects_sample))
        if usegpu:
            margin = margin.cuda()
        margin = Variable(margin)

        _dist_term_sample = torch.sum(
            torch.clamp(margin - _norm, min=0.0) ** 2)
        _dist_term_sample = _dist_term_sample / \
            (_n_objects_sample * (_n_objects_sample - 1))
        dist_term += _dist_term_sample

    dist_term = dist_term / bs

    return dist_term

'''def calculate_distance_term(means, n_objects, delta_d, norm=2, usegpu=True):
    """means: bs, n_instances, n_filters"""

    bs, n_instances, n_filters = means.size()

    dist_term = 0.0
    
    _mean_samples = means[n_objects>1]
    #print(n_objects)
    n_objects = n_objects[n_objects>1]
    if len(n_objects) == 0:
        print(dist_term)
        return dist_term
        
    diff = _mean_samples.unsqueeze(1) - _mean_samples.unsqueeze(2)
    _norm = torch.norm(diff,norm,2)
    
    margins = torch.zeros(n_objects.size(0),n_instances,n_instances)
    for i,n_object in enumerate(n_objects):
    #    bs, height * width, n_instances)
        margins[i,:n_object,:n_object] = 1
    margins = margins.cuda()
    _dist_term_samples = torch.sum(torch.clamp(2*delta_d*margins - norm,min = 0.0 )**2) 
    #print("_dist",_dist_term_samples,(torch.sum(margins) - torch.sum(n_objects.float())))
    dist_term = _dist_term_samples/(torch.sum(margins) - torch.sum(n_objects.float()))
    return dist_term'''

def calculate_regularization_term(means, n_objects, norm):
    """means: bs, n_instances, n_filters"""

    bs, n_instances, n_filters = means.size()

    reg_term = 0.0
    for i in range(bs):
        _mean_sample = means[i, : n_objects[i], :]  # n_objects, n_filters
        _norm = torch.norm(_mean_sample, norm, 1)
        reg_term += torch.mean(_norm)
    reg_term = reg_term / bs

    return reg_term

'''def calculate_regularization_term(means, n_objects, norm):
    """means: bs, n_instances, n_filters"""

    bs, n_instances, n_filters = means.size()

    reg_term = 0.0
    
    #for i in range(bs):
    #    _mean_sample = means[i, : n_objects[i], :]  # n_objects, n_filters
    #    _norm = torch.norm(_mean_sample, norm, 1)
    #    reg_term += torch.mean(_norm)
    _norm = torch.norm(means,norm,-1)
    reg_term = torch.sum(_norm)/torch.sum(n_objects.float())
    #reg_term = reg_term / bs

    return reg_term'''


def discriminative_loss(input, target, n_objects,
                        max_n_objects, delta_v, delta_d, norm, usegpu):
    """input: bs, n_filters, fmap, fmap
       target: bs, n_instances, fmap, fmap
       n_objects: bs"""

    #print("n_objects:",n_objects)
    alpha = beta = 1.0
    gamma = 0.001

    bs, n_filters, height, width,depth = input.size()
    n_instances = target.size(1)

    
    input = input.view(bs,n_filters,-1)
    input = input.permute(0,2,1)

    target = target.view(bs,n_instances,-1)
    target = target.permute(0,2,1)
    
    


    cluster_means = calculate_means(
        input, target, n_objects, max_n_objects, usegpu)

    var_term = calculate_variance_term(
        input, target, cluster_means, n_objects, delta_v, norm)
    dist_term = calculate_distance_term(
        cluster_means, n_objects, delta_d, norm, usegpu)
    reg_term = calculate_regularization_term(cluster_means, n_objects, norm)

    loss = alpha * var_term + beta * dist_term + gamma * reg_term

    return loss


class DiscriminativeLoss(_Loss):

    def __init__(self, delta_var, delta_dist, norm,
                 size_average=True, reduce=True, usegpu=True):
        super(DiscriminativeLoss, self).__init__(size_average)
        self.reduce = reduce
        self.size_average = size_average

        assert self.size_average
        assert self.reduce

        self.delta_var = float(delta_var)
        self.delta_dist = float(delta_dist)
        self.norm = int(norm)
        self.usegpu = usegpu

        assert self.norm in [1, 2]

    def forward(self, input, target, n_objects, max_n_objects):
        return discriminative_loss(input, target, n_objects, max_n_objects,
                                   self.delta_var, self.delta_dist, self.norm,
                                   self.usegpu)
